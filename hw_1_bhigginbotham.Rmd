---
title: "HW_1_bhigginbotham"
author: "brian higginbotham"
date: "2024-07-05"
urlcolor: blue
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warnings = FALSE, error = FALSE)
```

## 11.22

```{r treadmill data}
treadmill <- read.table(file="data/fitness.txt", header=TRUE)
```

### a) plot the data

```{r plot treadmill}
plot(treadmill$treadmill, treadmill$tenk)
```

### b) Fit regression model to the data

```{r fit regression}
slr_tread <- lm(tenk ~ treadmill, data=treadmill)
plot(treadmill$treadmill, treadmill$tenk)
abline(slr_tread, col=2)
```
The linear model does seem appropriate - there is an overall downward trend in the data that is reflected in the model.

### c) estimate the linear regression model

```{r model summary}
summary(slr_tread)
```
We see from the summary output that the estimates for the linear model are `b_hat_0 (Intercept) = 58.8158` and `b_hat_1 (slope) = -1.8673`. Therefore the estimate for the linear regression model would be

`y_hat = 58.8158 - 1.8673x_i + e` for any given `i`. `e` represents the error or unknowns that were not accounted for in the data.

## 11.23

### a) estimate population variance for errors (sigma^2_e)

`Residual Standard Error (RSE) = 2.102` - from model summary  
`RSE = sqrt(sigma^2_e)`  
`RSE^2 = sigma^2_e`
```{r error variance}
RSE <- 2.102
RSE**2
```
Estimate for error variance

### b) estimate standard error for b_hat_1

`0.3462` - from summary output: Std. Error for 'treadmill'

### c) 95% Confidence Interval for b_hat_1
```{r b_hat_1 CI}
confint(slr_tread, 'treadmill', level=0.95)
```
the plausible values of b_1, given a significance level of 0.05 would be
`-2.59 < b_1 < -1.140`

### d) hypothesis test that there is a linear relationship between treadmill time and 10K time
`Pr(>|t|)` for 'treadmill' from summary output = `3.99e-05` - near zero percent chance of the null hypothesis (b_hat_1 = 0) occuring for the t value `-5.393` in the student t-distribution.  
**Reject H_0** - conclude there is a linear relationship between treadmill time and 10K time

## 11.36
### a) Estimate the mean time to run 10km for athletes having a treadmill time of 11 minutes.
From 11.22c, we noted that the linear regression model was `y_hat = 58.8158 - 1.8673x_i + e`. Since we assume that the errors have a mean of `0`, we can estimate the mean response (time to run 10km) by using the value `11` for `x_i`.
```{r estimate mean time}
mu_new <- 58.8158 - 1.8673*11
mu_new
```
### b) Place a 95% CI on the mean time to run 10km for the above estimate
Here we'll use the `predict()` function and set the *interval* argument to **'conf'** and *level* argument to **'0.95'**
```{r CI for mean estimate}
predict(slr_tread, data.frame(treadmill=11), interval="conf", level =0.95)
```
`fit` is the mean response and `lwr` and `upr` are the bounds of the **95%** CI.

## 11.37
### a) Predict the time to run 10km if an athlete has a treadmill time of 11 minutes
Predictions will use the same regression model formula as estimated means. We cannot use the assumed error average (0) to nullify the error term, but we do assume that the error portion of a new prediction is close to `0`, so the predicted value will equal the average value: `y_new = mu_new`.
```{r predicted value}
y_new <- 58.8158 - 1.8673*11
y_new
```
### b) Place a 95% PI (Prediction Interval) on the above prediction
We'll use the same `predict()` function but set the *interval* argument to **'pred'**. The *level* argument will remain **'0.95'**.
```{r PI for prediction estimate}
predict(slr_tread, data.frame(treadmill=11), interval="pred", level =0.95)
```
`fit` is the predicted response and `lwr` and `upr` are the bounds of the **95%** CI.  

### c) Compare the PI to CI in 11.36. What is the difference in the interpretation of the two intervals? Why is one interval wider than the other?

The estimated mean is the average of a collection of data were treadmill time equals 11. As such, we would expect a similar average to occur in repeated studies with some variability. The CI for the estimated mean reflects this variability. The predicted value is an unknown and represents the possible time to complete 10km for any one individual whose treadmill time is 11 minutes. As such the prediction will have additional variance associated with it. This additional variance accounts for the wider interval in the PI than the CI.

This makes sense when we think about the comparison between averages and the individual datum that make up those averages. The variability between averages of similar measurements will be less than the variability of predicting what one of those measurements is.

## 11.40

```{r drug data}
drug <- read.table(file="data/drug.txt", header=TRUE)
drug
```

### a) Plot the Data
```{r plot drug}
y<-drug$response
x<-drug$dose
plot(x,y)
```

### b) Fit a Linear Regression Model
```{r fit drug SLR}
slr_drug <- lm(y ~ x)
summary(slr_drug)
plot(x,y)
abline(slr_drug, col=4)
```

### c) Plot the residuals and evaluate model fit

We'll first check linearity by plotting the residuals vs. the y_hats (fitted values). If there is a linear relationship between the two, then we would expect the residuals to be randomly scattered about the plot, forming no discernable pattern.
```{r residual plots}
plot(slr_drug$fitted.values, slr_drug$residuals)
abline(h=0, col=6)
```
Although the data is limited, we can see from the plot that the residuals are not evenly or randomly scattered about. The residuals take on a kind of arch pattern - starting at negative and increasing to the mid-way point along the x-axis and then decreasing again into the negative.

From this plot, we should be aware that there is a possible lack of fit in the linear model to the data.

## 11.42
Transform `dose` variable to the natural log and re-evaluate the data
```{r x transform}
xi<-log(x)
```

### a) Plot the transformed data
```{r plot drug transform}
plot(xi,y)
```
Compared to our previous plot from `11.40a`, there is definitely a more linear pattern to the data.

### b) Fit linear regression model
```{r slr drug transform}
slr_drug2 <- lm(y ~ xi)
summary(slr_drug2)
plot(xi, y)
abline(slr_drug2, col=4)
```

### c) Plot the residuals
```{r drug transform residual plot}
plot(slr_drug2$fitted.values, slr_drug2$residuals)
abline(h=0, col=6)
```
Here, we see a more random scattering of the residuals than before. There is no discernible pattern in the plot which backs up our assumption of linearity.

Let's check to see if the residuals are normally distributed using a `QQ-Plot`.
```{r drug transform QQPlot}
qqnorm(slr_drug2$residuals)
qqline(slr_drug2$residuals)
```
There are two points that deviate from the line towards the edge of the plot which can be expected in a quantile plot. Otherwise, the residuals follow pretty closely to the normal distribution, which would be expected for our linear assumption.

### d) Conduct a test for lack of fit of the linear regression model
This test was done for us in the `lm()` fit. The test for lack of fit proposes the null hypothesis to be `beta_hat_1 = 0` - i.e. there is no linear or slope relationship between the variables. To view the results of this test, we'll reprint the model summary.
```{r drug transform model summary}
summary(slr_drug2)
```
From the Coefficients section we can see the calculated **t-value** and **p-score** of `beta_hat_1` (`xi`).
`Pr(>|t|)` for 'xi' from summary output = `2.95e-10` - near zero percent chance of the null hypothesis (beta_hat_1 = 0) occuring for the t value `16.985` in the student t-distribution.  
**Reject H_0** - conclude there is a linear relationship between the natural log of the dose and the response.